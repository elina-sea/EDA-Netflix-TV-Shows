---
title: "SECOND HAND CARS DATA SET"
output:
  html_document:
    df_print: paged
editor_options: 
  chunk_output_type: inline
---
#### comment
  Written with some trivial explanations and conclusions in order go through the process of explaining everything to myself for remembering objects better.


# Part 1

  This is an R Notebook containing the analysis of 1000 cars on second-hand market. Data set was obtained from [Kaggle]https://www.kaggle.com/mayankpatel14/second-hand-used-cars-data-set-linear-regression).
  
  Description from author:
  This is a dataset that can be used to apply regression algorithms for beginners. There are no missing value and the data set is small too.
  The data set gives the second hand price of cars and its features like:
  
- v.id
- on road old
- on road now
- years
- km
- rating
- condition
- economy
- top speed
- hp
- torque
- current price
  
  
  Let's start with observing what types of variables there are in this data set:

```{r echo = FALSE}
library(readr)
library(ggplot2)
#read in the dataset
cars <- read_csv("cars.csv")
```

  As we can observe this data set has only quantitative variables and they all are represented by numeric values of type double.
  
  
### Analysing missing values 

```{r message=FALSE, paged.print=FALSE, echo=FALSE}
library(naniar)
#plot missing values with naniar pkg which is based on ggplot2
gg_miss_var(cars)
```

  As we can see, there are no missing values in this data set. This means we can compute further relations without complications.
  
## Now let's have fun and explore distribution of each parameter individually

  On the following graphs distribution of each parameter is shown.
  
  
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(cowplot)
library(dplyr)
library(hrbrthemes)

km <- c(cars$km)
years <- c(cars$years)
rating <- c(cars$rating)
`condition rating` <- c(cars$condition)
discount <- c(cars$economy)
mph <- c(cars$`top speed`)
hp <- c(cars$hp)
torque <- c(cars$torque)
price <- c(cars$`current price`)

df_cars <- data.frame(cars)

ggplot(df_cars, aes(km)) + geom_histogram(aes(y = ..density..)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.2) +
  ggtitle("Distribution of car mileage") +
  theme_ipsum()
ggplot(df_cars, aes(years)) + geom_histogram(aes(y = ..density..)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.2) +
  ggtitle("Distribution of car age") +
  theme_ipsum()
ggplot(df_cars, aes(rating)) + geom_histogram(aes(y = ..density..)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.2) +
  ggtitle("Distribution of car rating from 1 to 5") +
  theme_ipsum()
ggplot(df_cars, aes(`condition rating`)) + geom_histogram(aes(y = ..density..)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.2) +
  ggtitle("Distribution of car condition rating from 1 to 10") +
  theme_ipsum()
ggplot(df_cars, aes(discount)) + geom_histogram(aes(y = ..density..)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.2) +
  ggtitle("Distribution of sale for car in %") +
  theme_ipsum()
ggplot(df_cars, aes(mph)) + geom_histogram(aes(y = ..density..)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.2) +
  ggtitle("Distribution of car maximum speed in miles/hour") +
  theme_ipsum()
ggplot(df_cars, aes(torque)) + geom_histogram(aes(y = ..density..)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.2) +
  ggtitle("Distribution of car torque") +
  theme_ipsum()
ggplot(df_cars, aes(price)) + geom_histogram(aes(y = ..density..)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.2) +
  ggtitle("Distribution of car price") +
  theme_ipsum()


```

### Summary on distribution graphs

  From the distribution graphs above we can conclude:
  
- there is approximately even ratio between amount of cars for each rating and "age" of a car
- most of the cars have middle price rather than low or high
  
  Speaking of graphs themselves, there are no skewed graphs - rather multimodal. Car price distribution looks close to normal bell-shaped distribution. Since it is the only one to have normal distribution we can target it later for regression.
  

# Part 2
  
  In this part we will concentrate on examining each parameter separately.
  
### Mean values
#### Mean of mileage
```{r  message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)

mean(km)
```
#### Mean of car age
```{r  message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)

mean(years)
```
#### Mean of rating
```{r  message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)

mean(rating)
```
#### Mean of condition rating
```{r  message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)

mean(`condition rating`)
```
#### Mean of given discount
```{r  message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)

mean(discount)
```
#### Mean of maximum speed
```{r  message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)

mean(mph)
```
#### Mean of torque
```{r  message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)

mean(torque)
```
#### Mean of price
```{r  message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)

mean(price)
```


### Mode values
#### Mode of mileage
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(DescTools)

Mode(km)
```
#### Mode of car age
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(DescTools)

Mode(years)
```
#### Mode of rating
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(DescTools)

Mode(rating)
```
#### Mode of condition rating
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(DescTools)

Mode(`condition rating`)
```
#### Mode of discount given
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(DescTools)

Mode(discount)
```
#### Mode of maximum speed
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(DescTools)

Mode(mph)
```
#### Mode of maximum torque
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(DescTools)

Mode(torque)
```
#### Mode of maximum price
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(DescTools)

Mode(price)
```


  Let's discuss mode a bit. As we can see, the result went a bit off for mileage mode. But actually it's not off but rather demonstrates some of the "disadvantages" of mode:
  
- mode is unstable when the data consist of a small number of values;
- data can have more than one mode or no mode at all;
    
  Though it is often considered to be a disadvantage, it's just another way to make some conclusions upon given data. If we have several values which are met with the same frequency we can conclude that distribution graph is meant to have several peaks. In other words by computing mode of mileage we can already say that the distribution graph of this value is meant to be multimodal. And if we go back to our mileage graph we will see that it's indeed multimodal.
  
  
```{r}
ggplot(df_cars, aes(km)) + geom_histogram(aes(y = ..density..)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.2) +
  ggtitle("Distribution of car mileage") +
  theme_ipsum()
```
  
  

### Median values
#### Median of mileage
```{r  message=FALSE, warning=FALSE, echo=FALSE}

median(km)
```
#### Median of car age
```{r  message=FALSE, warning=FALSE, echo=FALSE}

median(years)
```
#### Median of rating
```{r  message=FALSE, warning=FALSE, echo=FALSE}

median(rating)
```
#### Median of condition rating
```{r  message=FALSE, warning=FALSE, echo=FALSE}

median(`condition rating`)
```
#### Median of given discount
```{r  message=FALSE, warning=FALSE, echo=FALSE}

median(discount)
```
#### Median of maximum speed
```{r  message=FALSE, warning=FALSE, echo=FALSE}

median(mph)
```
#### Median of torque
```{r  message=FALSE, warning=FALSE, echo=FALSE}

median(torque)
```
#### Median of price
```{r  message=FALSE, warning=FALSE, echo=FALSE}

median(price)
```
  
### Variabilty
#### Mileage
```{r}
Range(km)
IQR(km)
var(km)
```

#### Car age
```{r}
Range(years)
IQR(years)
var(years)
```

  [DELETE]  
  From the computations above we can say:
    - luminosity of each star in the set lays in a range between 0.8 and 849420 lumens
    - range between Q1 and Q equals 198050
    - variance or average squared difference of the scores from the mean equals 32195930524 - it is a very big value which indicates that numbers in set are far from mean and far from each other **or** it might also state that there are some outlining values which add great weight to this number. 
    So let's `trim` luminosity and try again. By the way, we could have trimmed luminosity, temperature and radius sets in the very beginning as good practice, because those distributions have extremely skewed graphs.
    [DELETE]  
  
  [DELETE]  
```{r}
```
   We removed 20% of lowest and greatest values and result is still a very large value what proves that deviations from mean are quite big for this parameter. 
     [DELETE]  

#### Discount
```{r}
Range(discount)
IQR(discount)
var(discount)
```  
 
  From the computations above we can say:
  
- radius of each star in the set lays in a range between 0.0084 and 1948.5 
- range between Q1 and Q3 equals 42.64725, compairing it o the main ramge we can conclude that major amount of records are below Q3
- variance or average squared difference of the scores from the mean equals 267450.1 - it is a  big value too, but not in terms of this data set


#### Maximum speed
```{r}
Range(mph)
IQR(mph)
var(mph)
```    
  
#### Torque
```{r}
Range(torque)
IQR(torque)
var(torque)
``` 
  [CONCLUSIONS]

  [CONCLUSIONS]

#### Price
```{r}
Range(price)
IQR(price)
var(price)
```   

  
---
# Part 3
  
  In this part we will explore some dependencies between known parameters of the stars in our dataset. 
  Let's choose one parameter which we will target in this part of work. This parameter will be out dependent value, which means we will explore how and how much it is influenced by other parameters. 
  **Current price** is going to be such parameter. It means we will investigate what price of a car in second-hand market depends on.

  
## Correlations and correlation matrix
  
  Purpose of this computation is to find what values correlate with price the most.
  
```{r}
library(corrplot)

cars_cor <- cor(cars)
corrplot(cars_cor, method ="pie", type="lower")
```
  
  
  As we can see observe from the correlation matrix above, the highest correlation is between mileage(km)-price pair, we can say that this values are connected and assume that they have cause-effect relationship. Their correlation coefficient is negative which means that with the growth of one parameter the other one lowers simultaneously. There are also several positive correlations (both arguments grow simultaneously). And though condition rating doesn't have very high correlation to price, I think it will be interesting to compute a scatter plot for it just to compare high/low correlation scatter plots.
  Let's take those three pairs into further steps and build scatter plots for them.
  
```{r}
library(ggplot2)
library(DescTools)
library(hrbrthemes)

ggplot(cars, aes(x = km, y = price)) + 
  geom_point() +
  ggtitle("Scatter plot: price vs mileage") +
  theme_ipsum()
ggplot(cars, aes(x = `condition rating`, y = price)) +
  geom_point() +
  ggtitle("Scatter plot: price vs condition rating") +
  theme_ipsum()

```

  As for price-mileage plot, we can clearly see that values correlate - graph has quite narrow span and there are no outliers. As for the price-condition rating - graph might look a bit off, but the reason is that rating consists of just natural numbers from 1 to 10 - so each step for growth is 1.0. But we still can see a tendency for those vertical lines to go up as they move towards 10.0, so there's a weak correlation.
  
## Regression

  Let's take price-mileage pair for further work.
  
```{r}

lm_model <- lm(price~km)
lm_model
summary(lm_model)

plot(price~km, ylim=c(0, 650000)) + 
  abline(lm_model,col="red", lwd=3) +
  title("Linear Regression: price vs mileage") +
  theme_ipsum()

```
  
  
  It's not a surprise that plot line proofs tendency and follows scatter plot, so let's devote more time to summary. 
  
#### Residuals
   From the first glance median is a large value and isn't close to 0 which states that positive residual is greater than negative (Min/Max), but considering we deal with prices like 300 000 it's not a big fluctuation. Lets see a graph of residuals to proof that:
  
  
```{r}
boxplot(lm_model[['residuals']], main='Boxplot: Residuals', ylab='residual value')
```
  
  As mentioned above, this boxplot proofs indeed that on the given scale fluctuation of median isn't critical and it's quite close to zero. Plus, upper and lower bounds are quite symmetrical too. This states that prediction can be considered as a trustworthy.
  
#### Coefficients
  **Estimates** tells us that when mileage tends to zero, price tends to the value of 700 000.
  **Standard interval** gives us the confidence interval. From the code below we can say with 97% accuracy that with mileage -3.953173 car price will be 724290.555115, but since negative mileage isn't possible - we go back to what was said in ***estimates*** part.

```{r}
confint(lm_model)
```
  
  [**t value**](https://en.wikipedia.org/wiki/T-statistic) tells us about how far our estimated parameter is from a hypothesized 0 value, scaled by the standard deviation of the estimate.
  **Pr(>|t|) ** is the [p-value](https://en.wikipedia.org/wiki/P-value) for the individual coefficient.

#### Residual standard error
  gives an idea of how large the prediction error is in the given sample. [More about degrees of freedom and how to read them.](https://www.investopedia.com/terms/d/degrees-of-freedom.asp)
  
#### Multiple R-squared
  or the Coefficient of Determination - coefficient showing how sure we can be about our predictions. 0.87 is a very high value which gives us the right to say predictions will be made with almost 90% accuracy.
  
### Summary

  With given data we can compute regression and predict price for cars on second-hand market. And with accuracy of 87% we can say that a car with almost zero mileage (or new car) would cost around 700k.
  
# Part 4: Lotto Super 7
  
  Lotto Super 7 was a national lottery game in Canada, operated by the Interprovincial Lottery Corporation. It was launched on June 10, 1994, and its last draw was on September 18, 2009. The lottery had a guaranteed jackpot of $2.5 million, which was carried forward to the next draw if no purchased ticket matched all seven numbers in that draw.
  To win a prize competitor had to guess a variation of combination of 7 numbers and additional bonus number out of 47 numbers (1-47 range). For every $2 spent, three selections of seven numbers were given. However not all combinations were leading to prize. Only 7 combinations were considered as a match. 
  They are the following (from greatest to least):
  
- 7/7
- 6/7+ (+ states for bonus number)
- 6/7
- 5/7
- 4/7
- 3/7+
- 3/7 (free ticket as a prize)
  
  We will use the following formula: n! / r! (n-r)!
  Where: 
  
-  n is amount of numbers to choose from (47 for Lotto Super)
-  r is amount of numbers we can choose

### Probabilities 

  Let's start from match with the highest probability and move towards least probable match.

#### 3/7 match

  First let's write a function which will help us un calculations:
  
```{r}
prob <- function(n, r) {
  res <- factorial(n) / (factorial(r) * factorial(n - r))
  return(res)
}
prob(n = 47, r = 3) / 3
```

  The probability of guessing the 3/7 match is `1 in 5405` 
  Note: we are dividing result by 3, because entering lottery once we can but 3 sets of numbers. So, the initial probability to guess 3/7 (or three of 47) is `1 in 16215`, but since we have 3 sets it becomes `3 in 16215` or `1 in 5405`. This logic will be followed in further calculations too. For cases when there's a bonus number we assume bonus number also gets to be chosen 3 times (1 for each set of 7 numbers).
 **Example for bonus ticket match:**
  
#### 3/7+
```{r}
prob(n = 47, r = 3) * prob(n = 47, r = 1) / 3
```
```{r message=FALSE, warning=FALSE, include=FALSE}
prob(n = 47, r = 4) / 3
prob(n = 47, r = 5) / 3
prob(n = 47, r = 6) / 3
prob(n = 47, r = 6) * prob(n = 47, r = 1) / 3
prob(n = 47, r = 7) / 3
```

  
### Summary 

  After completing calculations we got the following results:
  
- 7/7 match - 1 : 20963833
- 6/7+ match - 1 : 168221977
- 6/7 match - 1 : 3579191
- 5/7 match - 1 : 511313
- 4/7 match - 1 : 59455
- 3/7+ match - 1 : 254035
- 3/7 match - 1 : 5405

  All results can be checked [here](https://www.lotterycritic.com/lottery-calculators/lottery-odds-calculator/#odds).
  
# Part 5

  In this part we will split our dataset into testing set and training set.
  
```{r}
#create a list of random number ranging from 1 to number of rows from actual data 
#and 70% of the data into training data  

cars_split = sort(sample(nrow(cars), nrow(cars)*.7))

#creating training data set by selecting the output row values
cars_train <- cars[cars_split, ]

#creating test data set by not selecting the output row values
cars_test <- cars[-cars_split, ]

cars_test
cars_train
```

  Now that we have our training and testing datasets we can proceed with training regression model on training set. Then we will compare the outcome against results of testing set.
  
#### Training set
```{r}
library(ggplot2)
library(DescTools)
library(hrbrthemes)

lm_train <- lm(cars_train$`current price`~cars_train$km)
summary(lm_train)

plot(cars_train$`current price`~cars_train$km, xlab = "km (training set)", ylab = "price (training set)", ylim=c(0, 650000) )
  abline(lm_model,col="red", lwd=3) +
  title("Linear Regression on Testing Set: Price vs Mileage") 
```
#### Testing set 

  Let's do regression for testing set and compare it to results of training set.
  
```{r}
lm_test <- lm(cars_test$`current price`~cars_test$km)
summary(lm_test)

plot(cars_test$`current price`~cars_test$km, xlab = "km (testing set)", ylab = "price (testing set)", ylim=c(0, 650000) )
  abline(lm_model,col="red", lwd=3) +
  title("Linear Regression on Training Set: Price vs Mileage") 
```

## Metrics
  We can see from the summaries that results are quite close (for example, estimates are almost the same), but for more precise evaluation we will apply some metrics on our testing set. Some of the metrics from summaries were discussed and explained in step 3, so we will omit their explanation here.
  
  
#### Root Mean Squared Error

  RMSE shows us the error rate of a model.
  
```{r}
library(qpcR)

RMSE(lm_test)
```
  
  From this calculation we can see that the difference between predicted and real prices will be 44348.31. Let's figure out whether it's a lot or not by calculating MAPE. 
  
#### Mean Absolute Percentage Error


```{r}
MAPE(lm_test)
```

 15.2% is an average difference between the predicted value and the actual value.
 
 
### Summary 
   Let's sum up the metrics of our testing set.
   **R squared** equals 0.8769 which means our predictions' accuracy is 87.69% (the same value as we had for our general model which states that testing model performs well). MAPE deviations are nor higher that 15.5% which is also a good result implying out predictions are reliable. 
  


























