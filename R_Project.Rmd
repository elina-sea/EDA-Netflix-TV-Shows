---
title: "SECOND HAND CARS DATA SET"
output:
  html_document:
    df_print: paged
  pdf_document: default
editor_options:
  chunk_output_type: inline
---
#### comment
  Written with some trivial explanations and conclusions in order go through the process of explaining everything to myself for remembering objects better.
#### end of comment

  This is an R Notebook containing the analysis of 1000 cars on second-hand market. Data set was obtained from [Kaggle](https://www.kaggle.com/mayankpatel14/second-hand-used-cars-data-set-linear-regression). 
  
  Description from author:
  This is a dataset that can be used to apply regression algorithms for beginners. There are no missing value and the data set is small too.
  The data set gives the second hand price of cars and its features like:
  
- v.id (simply ID, which increments)
- on road old (unfortunately, author hasn't provided explanation on this value)
- on road now (unfortunately, author hasn't provided explanation on this value)
- years (how old is vehicle)
- km (mileage of the car)
- rating (simply rating from 1 to 5)
- condition (one more rating from 1 to 10)
- economy (dicount for car)
- top speed (what speed can vehicle develop)
- hp (horse power)
- torque 
- current price
  
# Worlflow

   In this paper we will explore each value separately (in parts 1-2) and see how they are distributed. We will also make some connections between central tendency measures-distribution graphs and between some variance values. In part 3 we will perform regression against targeted value and we will try to predict its future values. Part 4 will be devoted to off-top lottery computations, and later in part 5 we will finalize our regression by evaluating testing dataset. 


# Part 1   
  Let's start with observing what types of variables there are in this data set:

```{r echo = FALSE}
library(readr)
library(ggplot2)
#read in the dataset
cars <- read_csv("cars.csv")
```

  As we can observe this data set has only quantitative variables and they all are represented by numeric values of type double.
  
  
### Analysing missing values 

```{r message=FALSE, paged.print=FALSE, echo=FALSE}
library(naniar)
#plot missing values with naniar pkg which is based on ggplot2
gg_miss_var(cars)
```

  As we can see, there are no missing values in this data set. This means we can compute further relations without complications.
  
## Now let's have fun and explore distribution of each parameter individually

  On the following graphs distribution of each parameter is shown.
  
  
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(cowplot)
library(dplyr)
library(hrbrthemes)

km <- c(cars$km)
years <- c(cars$years)
rating <- c(cars$rating)
`condition rating` <- c(cars$condition)
discount <- c(cars$economy)
mph <- c(cars$`top speed`)
hp <- c(cars$hp)
torque <- c(cars$torque)
price <- c(cars$`current price`)

df_cars <- data.frame(cars)

ggplot(df_cars, aes(km)) + geom_histogram(aes(y = ..density..)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.2) +
  ggtitle("Distribution of car mileage") +
  theme_ipsum()
ggplot(df_cars, aes(years)) + geom_histogram(aes(y = ..density..)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.2) +
  ggtitle("Distribution of car age") +
  theme_ipsum()
ggplot(df_cars, aes(rating)) + geom_histogram(aes(y = ..density..)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.2) +
  ggtitle("Distribution of car rating from 1 to 5") +
  theme_ipsum()
ggplot(df_cars, aes(`condition rating`)) + geom_histogram(aes(y = ..density..)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.2) +
  ggtitle("Distribution of car condition rating from 1 to 10") +
  theme_ipsum()
ggplot(df_cars, aes(discount)) + geom_histogram(aes(y = ..density..)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.2) +
  ggtitle("Distribution of sale for car in %") +
  theme_ipsum()
ggplot(df_cars, aes(mph)) + geom_histogram(aes(y = ..density..)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.2) +
  ggtitle("Distribution of car maximum speed in miles/hour") +
  theme_ipsum()
ggplot(df_cars, aes(torque)) + geom_histogram(aes(y = ..density..)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.2) +
  ggtitle("Distribution of car torque") +
  theme_ipsum()
ggplot(df_cars, aes(price)) + geom_histogram(aes(y = ..density..)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.2) +
  ggtitle("Distribution of car price") +
  theme_ipsum()


```

### Summary on distribution graphs

  From the distribution graphs above we can conclude:
  
- there is approximately an even ratio between amount of cars for each rating and "age" of a car
- most of the cars have middle price rather than low or high
  
  Speaking of graphs themselves, there are no skewed graphs - rather multimodal. Car price distribution looks close to normal bell-shaped distribution. Since it is the only one to have normal distribution we can target it later for regression.
  

# Part 2
  
  In this part we will concentrate on examining each parameter separately.
  
### Mean values

  Mean is a simple value which is calculated by taking the sum of the values and dividing it by the number of records in dataset.
#### Mean of mileage
```{r echo=TRUE, message=FALSE, warning=FALSE}
library(ggplot2)
# the example of code will be shown once
mean(km)
```
#### Mean of car age
```{r  message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)

mean(years)
```
#### Mean of rating
```{r  message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)

mean(rating)
```
#### Mean of condition rating
```{r  message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)

mean(`condition rating`)
```
#### Mean of given discount
```{r  message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)

mean(discount)
```
#### Mean of maximum speed
```{r  message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)

mean(mph)
```
#### Mean of torque
```{r  message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)

mean(torque)
```
#### Mean of price
```{r  message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)

mean(price)
```


### Mode values
  
  Mode is the value which occurs with the highest frequency.
#### Mode of mileage
```{r echo=TRUE, message=FALSE, warning=FALSE}
library(DescTools)
# the example of code will be shown once
Mode(km)
```
This computations show that values 52701, 130004, 132215 are the greatest values among mileage records. And all of the met with frequency = 2. 3 values for mode is a bit unusual, but we will discuss it further.

#### Mode of car age
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(DescTools)

Mode(years)
```
#### Mode of rating
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(DescTools)

Mode(rating)
```
  
  Here we can see how tricky mode might be. From the first glance and not knowing what is the mean or median we could say that most of the cars have rating 4/5, but considering that the total amount of cars is 1000 we know for sure it's not the major value.
  
#### Mode of condition rating
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(DescTools)

Mode(`condition rating`)
```
#### Mode of discount given
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(DescTools)

Mode(discount)
```
#### Mode of maximum speed
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(DescTools)

Mode(mph)
```
#### Mode of maximum torque
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(DescTools)

Mode(torque)
```
#### Mode of maximum price
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(DescTools)

Mode(price)
```


  Now let's get back to mileage and price mode. Why there are three values instead of 1? And why price has no mode? To understand it we should refer to the properties of mode:
  
- mode is unstable when the data consist of a small number of values;
- data can have more than one mode or no mode at all - this is the case for price. It means no price occurs twice in set;
- if there are several mode values which are met with the same frequency we can conclude that distribution graph is meant to have several peaks.
    
  Last statement is the case for mileage. In other words, by computing the mode of mileage we could already say that the distribution graph of this value is meant to be multimodal. And vice versa. Let's we go back to mileage distribution graph and we will see that it's indeed multimodal.
  
  
```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot(df_cars, aes(km)) + geom_histogram(aes(y = ..density..)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.2) +
  ggtitle("Distribution of car mileage") +
  theme_ipsum()
```
  
  

### Median values

  Median is the value which equals to 50th percentile in set.
#### Median of mileage
```{r  message=TRUE, warning=FALSE, echo=FALSE}
# the example of code will be shown once
median(km)
```
#### Median of car age
```{r  message=FALSE, warning=FALSE, echo=FALSE}

median(years)
```
#### Median of rating
```{r  message=FALSE, warning=FALSE, echo=FALSE}

median(rating)
```
#### Median of condition rating
```{r  message=FALSE, warning=FALSE, echo=FALSE}

median(`condition rating`)
```
#### Median of given discount
```{r  message=FALSE, warning=FALSE, echo=FALSE}

median(discount)
```
#### Median of maximum speed
```{r  message=FALSE, warning=FALSE, echo=FALSE}

median(mph)
```
#### Median of torque
```{r  message=FALSE, warning=FALSE, echo=FALSE}

median(torque)
```
#### Median of price
```{r  message=FALSE, warning=FALSE, echo=FALSE}

median(price)
```
  
  Let's note that median of price is very close to the mean of price which equals 308520.2. This means that values in set are ~equally spaced. Again, it points to the shape of distribution. 
  
### Variabilty
  First calues to be computed is range, the interquartile range, then variance (or average squared difference).
#### Mileage
```{r}
Range(km)
IQR(km)
var(km)
```
  From the computations above we can say:
    - mileage of each star in the set lays in a range between 50324 and 149902 KMs
    - range between Q1 and Q equals 50680.5
    - variance or average squared difference of the scores from the mean equals 849749507 - what is a very big value which indicates that numbers in set are far from the mean value and far from each other **or** it might also state that there are some outlining values which add great weight to this number. 
    So let's `trim` mileage and try again.

```{r}
Range(Trim(km, trim = 0.2))
IQR(Trim(km, trim = 0.2))
var(Trim(km, trim = 0.2))
```
   We removed 20% of lowest and greatest values and result is still a very large value what proves that deviations from mean are quite big for this parameter. We can conclude that values in this set are not equally spaced. Most of them are much higher or lower than the mean value. Also we can connect it with range: mileage has a great range therefor it has a great variance too.

#### Car age
```{r}
Range(years)
IQR(years)
var(years)
```

#### Discount
```{r}
Range(discount)
IQR(discount)
var(discount)
```  

#### Maximum speed
```{r}
Range(mph)
IQR(mph)
var(mph)
```    
  
#### Torque
```{r}
Range(torque)
IQR(torque)
var(torque)
``` 

#### Price
```{r}
Range(price)
IQR(price)
var(price)
```   

#### Summary
  Now we have explored all values separately and know how to interpret, interconnect and read distribution, central tendency and variability computations. It also helped us to understand what parameter to target in regression - price. 
  
---
# Part 3
  
  In this part we will explore some dependencies between known parameters of cars in dataset. 
  We have already chosen the parameter which will ne our targeted one. This parameter will be a dependent value, which means we will explore how and how much it is influenced by other parameters. 
  We will investigate what **Current price**  of a car in second-hand market depends on and we will try to predict how it might change.

  
## Correlations and correlation matrix
  
  Purpose of this computation is to find what values correlate with price the most.
  
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(corrplot)

cars_cor <- cor(cars)
corrplot(cars_cor, method ="pie", type="lower")
```
  
  
  As we can see observe from the correlation matrix above, the highest correlation is between mileage(km)-price pair, we can say that this values are connected and assume that they have cause-effect relationship. Their correlation coefficient is negative which means that with the growth of one parameter the other one lowers simultaneously. There are also several positive correlations (both arguments grow simultaneously).
  Also we can note an interesting fact: neither rating nor condition rating correlate with anything significantly. This means they were given rather spontaneously by someone without basing on cars' real qualities.
  
  
## Scatter plots

  Let's plot the most important for us scatter plot: price-mileage. It will reveal whether they really correlate or there are outliers to blame. 
  And also though condition rating doesn't have very high correlation to price, I think it will be interesting to compute a scatter plot for it just to compare high/low correlation scatter plots.
  
  
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(DescTools)
library(hrbrthemes)

ggplot(cars, aes(x = km, y = price)) + 
  geom_point() +
  ggtitle("Scatter plot: price vs mileage") +
  theme_ipsum()
ggplot(cars, aes(x = `condition rating`, y = price)) +
  geom_point() +
  ggtitle("Scatter plot: price vs condition rating") +
  theme_ipsum()

```

  As for price-mileage plot, we can clearly see that values correlate - graph has quite narrow span and there are no outliers. As for the price-condition rating - graph might look a bit off, but the reason is that rating consists of just natural numbers from 1 to 10 - so each step for growth is 1.0. But we still can see a tendency for those vertical lines to go up as they move towards 10.0, so there's a weak correlation.
  
## Regression

  Let's take price-mileage pair for further work.
  
```{r echo=FALSE}

lm_model <- lm(price~km)
lm_model
summary(lm_model)

plot(price~km, ylim=c(0, 650000)) + 
  abline(lm_model,col="red", lwd=3) +
  title("Linear Regression: price vs mileage") +
  theme_ipsum()

```
  
  
  It's not a surprise that plot line proofs tendency and follows scatter plot, so let's devote more time to summary. 
  
#### Residuals
   From the first glance `median` seems to be a large value far above 0 (zero is what we aim for). This states positive residual is greater than negative (Min/Max), but considering we deal with prices like 300 000 it's not a big fluctuation. Lets see a graph of residuals to proof that:
  
  
```{r}
boxplot(lm_model[['residuals']], main='Boxplot: Residuals', ylab='residual value')
```
  
  As mentioned above, this boxplot proofs indeed that on the given scale fluctuation of median isn't critical and it's quite close to zero line. Plus, upper and lower bounds are quite symmetrical too. This states that prediction can be considered as a trustworthy.
  
#### Coefficients
  **Estimates** tells us that when mileage tends to zero, price tends to the value of ~700 000.
  **Standard interval** gives us the confidence interval. From the code below we can say with 97% accuracy that with mileage -3.953173 car price will be 724290.555115, but since negative mileage isn't possible - we go back to what was said in ***estimates*** part.

```{r}
confint(lm_model)
```
  
  [**t value**](https://en.wikipedia.org/wiki/T-statistic) tells us about how far our estimated parameter is from a hypothesized 0 value, scaled by the standard deviation of the estimate.
  **Pr(>|t|) ** is the [p-value](https://en.wikipedia.org/wiki/P-value) for the individual coefficient.

#### Residual standard error
  gives an idea of how large the prediction error is in the given sample. 5000 is a insignificant values for such a grest values. [More about degrees of freedom and how to read them.](https://www.investopedia.com/terms/d/degrees-of-freedom.asp)
  
#### Multiple R-squared
  or the Coefficient of Determination - coefficient showing how sure we can be about our predictions. 0.87 is a very high value which gives us the right to say predictions will be made with almost 90% accuracy.
  
### Summary

  With given data we can compute regression and predict price for cars on second-hand market. And with accuracy of 87% we can say that a car with almost zero mileage (or a new car which somehow ended up on a second-hand market...) would cost around 700 000.
  
  
# Part 4: Lotto Super 7
  
  Lotto Super 7 was a national lottery game in Canada, operated by the Interprovincial Lottery Corporation. It was launched on June 10, 1994, and its last draw was on September 18, 2009. The lottery had a guaranteed jackpot of $2.5 million, which was carried forward to the next draw if no purchased ticket matched all seven numbers in that draw.
  To win a prize competitor had to guess a variation of combination of 7 numbers and additional bonus number out of 47 numbers (1-47 range). For every $2 spent, three selections of seven numbers were given. However not all combinations were leading to prize. Only 7 combinations were considered as a match. 
  They are the following (from greatest to least):
  
- 7/7
- 6/7+ (+ states for bonus number)
- 6/7
- 5/7
- 4/7
- 3/7+
- 3/7 (free ticket as a prize)
  
  We will use the following formula: n! / r! (n-r)!
  Where: 
  
-  n is amount of numbers to choose from (47 for Lotto Super)
-  r is amount of numbers we can choose

### Probabilities 

  Let's start from match with the highest probability and move towards least probable match.

#### 3/7 match

  First let's write a function which will help us un calculations:
  
```{r}
prob <- function(n, r) {
  res <- factorial(n) / (factorial(r) * factorial(n - r))
  return(res)
}
prob(n = 47, r = 3) / 3
```

  The probability of guessing the 3/7 match is `1 in 5405` 
  Note: we are dividing result by 3, because entering lottery once we can but 3 sets of numbers. So, the initial probability to guess 3/7 (or three of 47) is `1 in 16215`, but since we have 3 sets it becomes `3 in 16215` or `1 in 5405`. This logic will be followed in further calculations too. For cases when there's a bonus number we assume bonus number also gets to be chosen 3 times (1 for each set of 7 numbers).
 **Example for bonus ticket match:**
  
#### 3/7+
```{r}
prob(n = 47, r = 3) * prob(n = 47, r = 1) / 3
```
```{r message=FALSE, warning=FALSE, include=FALSE}
prob(n = 47, r = 4) / 3
prob(n = 47, r = 5) / 3
prob(n = 47, r = 6) / 3
prob(n = 47, r = 6) * prob(n = 47, r = 1) / 3
prob(n = 47, r = 7) / 3
```

  
### Summary 

  After completing calculations we got the following results:
  
- 7/7 match - 1 : 20963833
- 6/7+ match - 1 : 168221977
- 6/7 match - 1 : 3579191
- 5/7 match - 1 : 511313
- 4/7 match - 1 : 59455
- 3/7+ match - 1 : 254035
- 3/7 match - 1 : 5405

  All results can be checked [here](https://www.lotterycritic.com/lottery-calculators/lottery-odds-calculator/#odds).
  
  
# Part 5

  In this part we will split our dataset into testing set and training set.
  
```{r}
#create a list of random number ranging from 1 to number of rows from actual data 
#and 70% of the data into training data  

cars_split = sort(sample(nrow(cars), nrow(cars)*.7))

#creating training data set by selecting the output row values
cars_train <- cars[cars_split, ]

#creating test data set by not selecting the output row values
cars_test <- cars[-cars_split, ]
```

  Now that we have our training and testing datasets we can proceed with training regression model on training set. Then we will compare the outcome against results of testing set.
  
#### Training set
```{r echo=FALSE, warning=FALSE}
library(ggplot2)
library(DescTools)
library(hrbrthemes)

lm_train <- lm(cars_train$`current price`~cars_train$km)
summary(lm_train)

plot(cars_train$`current price`~cars_train$km, xlab = "km (training set)", ylab = "price (training set)", ylim=c(0, 650000) )
  abline(lm_model,col="red", lwd=3) +
  title("Linear Regression on Testing Set: Price vs Mileage") 
```
#### Testing set 

  Let's do regression for testing set and compare its results to the results of training set.
  
```{r}
lm_test <- lm(cars_test$`current price`~cars_test$km)
summary(lm_test)

plot(cars_test$`current price`~cars_test$km, xlab = "km (testing set)", ylab = "price (testing set)", ylim=c(0, 650000) )
  abline(lm_model,col="red", lwd=3) +
  title("Linear Regression on Training Set: Price vs Mileage") 
```

## Metrics
  We can see from the summaries that results are quite close (for example, estimates are almost the same), but for precise evaluation of our testing set we will apply some metrics on it. Some of the metrics, which come from `summary`, were discussed and explained in step 3, so we will omit their explanation here.
  
  
#### Root Mean Squared Error

  RMSE shows us the error rate of a model.
  
```{r message=FALSE, warning=FALSE}
library(qpcR)

RMSE(lm_test)
```
  
  From this calculation we can see that the difference between predicted and real prices will be 44348.31. Let's figure out whether it's a lot or not by calculating MAPE. 
  
#### Mean Absolute Percentage Error


```{r}
MAPE(lm_test)
```

 15.2% is an average difference between the predicted value and the actual value.
 
 
### Summary 
   Let's sum up the metrics of our testing set.
   **R squared** equals 0.8769 which means our predictions' accuracy is 87.69% (the same value as we had for our general model which states that testing model performs well). **MAPE** deviations are not higher that 15.5% which is also a good result implying out predictions are reliable. This in general means that we can once more with accuracy 87% say that the price of a car with almost zero mileage will equal ~700000.
  


























